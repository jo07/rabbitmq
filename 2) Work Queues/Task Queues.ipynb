{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example in the testing folder we had one worker only to consume all the tasks. To scale the system we will need multiple workers to share the tasks. We will be creating multiple workers and they will be consuming the in a round robin fashion. That helps in parallelizing the tasks to all the worker nodes  done internally by RabbitMQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mock the task we will be using time.sleep based on number of dots each message has. So a task message \"message...\" will take 3 seconds to finish and so on. Let's make the changes to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send.py\n",
    "# Producer takes in an argument\n",
    "import sys\n",
    "import pika\n",
    "\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue='hello')\n",
    "def sendQueue():\n",
    "    try:\n",
    "        while True:\n",
    "            # keep taking input till kernel is interrupted\n",
    "            message = input(\"Enter new task\")\n",
    "            channel.basic_publish(exchange='',\n",
    "                                  routing_key='hello',\n",
    "                                  body=message)\n",
    "            print(\" [x] Sent %r\" % message)\n",
    "    except KeyboardInterrupt:\n",
    "        # Exit gracefully\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter new taskm.\n",
      " [x] Sent 'm.'\n",
      "Enter new taskm..\n",
      " [x] Sent 'm..'\n",
      "Enter new taskm...\n",
      " [x] Sent 'm...'\n",
      "Enter new taskm....\n",
      " [x] Sent 'm....'\n",
      "Enter new taskm.....\n",
      " [x] Sent 'm.....'\n",
      "Enter new taskm......\n",
      " [x] Sent 'm......'\n"
     ]
    }
   ],
   "source": [
    "sendQueue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ctag1.ec88106dcaad42f5b03e1fd2f8383dd3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recieve.py\n",
    "# consumer takes in the message and counts the dots in it to mock the task duration. It sleeps for seconds as much dots are in the message\n",
    "import time\n",
    "import pika\n",
    "\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue='hello')\n",
    "\n",
    "def callback(ch, method, properties, body):\n",
    "    print(\" [x] Received %r\" % body.decode())\n",
    "    time.sleep(body.count(b'.'))\n",
    "    print(\" [x] Done\")\n",
    "    \n",
    "channel.basic_consume(queue='hello', auto_ack=True, on_message_callback=callback)\n",
    "\n",
    "try:\n",
    "    channel.start_consuming()\n",
    "except KeyboardInterrupt:\n",
    "    # Exit gracefully\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need 3 shells to run this thing. Two worker nodes and one producer node. We will use this notebook as the producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So the tasks got divided among both the workers equally. The good thing about round robin approach is its parallize the task load. If our task queue gets overwhelming just add new worker node and the task gets split among them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the downside of this approach. The round robin distributes tasks in a sequence and marks for deletion. So what happens when one of the worker node working on a time consuming task goes down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task doesn't get completed and the pending task on that worker node which were dispatched and marked for deletion by the producer node also gets removed before completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message ACKnowledgement to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is simple. The rabbitmq server will wait for an acknowledgement from each of the worker nodes for each task that the task is consumed and processed so rabbitmq is free to delete it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose one of the worker nodes goes down for all the reason a node can go down, rabbitmq server will still be listening for the acknowledgement and after waiting for a set period of time (timeout) rabbitmq re-queue the task and distribute it to active nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *This way you make sure no task is left undone or message undelivered. So please acknowledge guys. This is very easy mistake with serious consequence. Like task queue can become full of unacknowledged messages and it will start eating more and more memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's update the callback funtion\n",
    "# Remove the default ack and sent a proper one. \n",
    "def callback(ch, method, properties, body):\n",
    "    print(\" [x] Received %r\" % body.decode())\n",
    "    time.sleep(body.count(b'.') )\n",
    "    print(\" [x] Done\")\n",
    "    #here\n",
    "    ch.basic_ack(delivery_tag = method.delivery_tag)\n",
    "# auto_ack=True flag is removed below.\n",
    "channel.basic_consume(queue='hello', on_message_callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if RabbitMQ server goes down and all the tasks with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='green'> Simple. Just tell rabbitmq to persist your messages. Let's see how to do that  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First of all you can't update the properties of an already defined queue. You will have to start with a new one*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.queue_declare(queue='task_queue', durable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *And since we don't know which worker or the producer node will start first we need to use this same code everywhere to declare this particular queue. This above step makes sure the 'task_queue' messages aren't lost when rabbitmq server restarts*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now to make the messages persistent we add the delivery_mode tag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.basic_publish(exchange='',\n",
    "                          routing_key=\"task_queue\",\n",
    "                          body=message,\n",
    "                          properties=pika.BasicProperties(\n",
    "                             delivery_mode = pika.spec.PERSISTENT_DELIVERY_MODE\n",
    "                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more thing to take care of. What if all the heavy duty tasks gets allocated to one worker and all the worker get less time intensive task and sit idle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RabbitMQ just distributes the task one after another to all the active worker nodes as soon as the tasks come in. It doesn't check wether the tasks are completed by a worker node or if a worker node is overloaded with tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair dispatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The straight forward solution is for the worker node to realise not to bite more than it can chew. Wait for the current task to get over then only take up new tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *basic_qos* channel with with 'prefetch_count' set as '1' does that exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel.basic_qos(prefetch_count=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting, new messages are assigned to worker nodes only after completing the current task. It makes sure no worker node is sitting idle and one particular node doing all the heavy duty when there are still pending list of tasks to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter new taskm.\n",
      "Enter new taskm..\n",
      "Enter new taskm...\n",
      "Enter new taskm....\n",
      "Enter new taskm.....\n",
      "Enter new taskm......\n",
      "Enter new taskm.\n",
      "Enter new taskm.\n",
      "Enter new taskm...\n",
      "Enter new taskm.\n",
      "Enter new taskm.....................\n",
      "Enter new taskm...\n"
     ]
    }
   ],
   "source": [
    "### Producer final code\n",
    "import pika\n",
    "import sys\n",
    "\n",
    "connection = pika.BlockingConnection(\n",
    "    pika.ConnectionParameters(host='localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "channel.queue_declare(queue='task_queue', durable=True)\n",
    "\n",
    "def sendQueue():\n",
    "    try:\n",
    "        while True:\n",
    "            # keep taking input till kernel is interrupted\n",
    "            message = input(\"Enter new task\")\n",
    "            channel.basic_publish(\n",
    "            exchange='',\n",
    "            routing_key='task_queue',\n",
    "            body=message,\n",
    "            properties=pika.BasicProperties(\n",
    "                delivery_mode=pika.spec.PERSISTENT_DELIVERY_MODE\n",
    "            ))\n",
    "    except KeyboardInterrupt:\n",
    "        # Exit gracefully\n",
    "        connection.close()\n",
    "\n",
    "sendQueue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## <font color='green'> Awesome </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "1. https://www.rabbitmq.com/documentation.html\n",
    "2. https://www.rabbitmq.com/confirms.html\n",
    "3. https://www.rabbitmq.com/ttl.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
