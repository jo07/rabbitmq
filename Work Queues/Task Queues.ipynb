{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last example in the testing folder we had one worker only to consume all the tasks. To scale the system we will need multiple workers to share the tasks. We will be creating multiple workers and they will be consuming the in a round robin fashion. That helps in parallelizing the tasks to all the worker nodes  done internally by RabbitMQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mock the task we will be using time.sleep based on number of dots each message has. So a task message \"message...\" will take 3 seconds to finish and so on. Let's make the changes to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send.py\n",
    "# Producer takes in an argument\n",
    "import sys\n",
    "import pika\n",
    "\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue='hello')\n",
    "def sendQueue():\n",
    "    try:\n",
    "        while True:\n",
    "            # keep taking input till kernel is interrupted\n",
    "            message = input(\"Enter new task\")\n",
    "            channel.basic_publish(exchange='',\n",
    "                                  routing_key='hello',\n",
    "                                  body=message)\n",
    "            print(\" [x] Sent %r\" % message)\n",
    "    except KeyboardInterrupt:\n",
    "        # Exit gracefully\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter new taskm.\n",
      " [x] Sent 'm.'\n",
      "Enter new taskm..\n",
      " [x] Sent 'm..'\n",
      "Enter new taskm...\n",
      " [x] Sent 'm...'\n",
      "Enter new taskm....\n",
      " [x] Sent 'm....'\n",
      "Enter new taskm.....\n",
      " [x] Sent 'm.....'\n",
      "Enter new taskm......\n",
      " [x] Sent 'm......'\n"
     ]
    }
   ],
   "source": [
    "sendQueue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ctag1.ec88106dcaad42f5b03e1fd2f8383dd3'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recieve.py\n",
    "# consumer takes in the message and counts the dots in it to mock the task duration. It sleeps for seconds as much dots are in the message\n",
    "import time\n",
    "import pika\n",
    "\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "channel.queue_declare(queue='hello')\n",
    "\n",
    "def callback(ch, method, properties, body):\n",
    "    print(\" [x] Received %r\" % body.decode())\n",
    "    time.sleep(body.count(b'.'))\n",
    "    print(\" [x] Done\")\n",
    "    \n",
    "channel.basic_consume(queue='hello', auto_ack=True, on_message_callback=callback)\n",
    "\n",
    "try:\n",
    "    channel.start_consuming()\n",
    "except KeyboardInterrupt:\n",
    "    # Exit gracefully\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need 3 shells to run this thing. Two worker nodes and one producer node. We will use this notebook as the producer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the tasks got divided among both the workers equally. The good thing about round robin approach is its parallize the task load. If our task queue gets overwhelming just add new worker node and the task gets split among them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the downside of this approach. The round robin distributes tasks in a sequence and marks for deletion. So what happens when one of the worker node working on a time consuming task goes down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task doesn't get completed and the pending task on that worker node which were dispatched and marked for deletion by the producer node also gets removed before completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message ACKnowledgement to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is simple. The rabbitmq server will wait for an acknowledgement from each of the worker nodes for each task that the task is consumed and processed so rabbitmq is free to delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Suppose one of the worker nodes goes down for all the reason a node can go down, rabbitmq server will still be listening for the acknowledgement and after waiting for a set period of time (timeout) rabbitmq re-queue the task and distribute it to "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
